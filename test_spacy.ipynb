{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test_spacy.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/rksharma55555/testspacy/blob/master/test_spacy.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "NaYJUfPzDzUy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install spacy\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5c3xh9g6nI3U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python -m spacy download en              # default English model (~50MB)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MzMr8QH1nuYR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!python -m spacy download en              # default English model (~50MB)\n",
        "!python -m spacy download en_core_web_md  # larger English model (~1GB)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VsxetgmZONOg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import spacy.cli\n",
        "spacy.cli.download(\"en_core_web_sm\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tbgVY6YKyqPE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eRpAiZVPOS9y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Installing en model\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(u'Apple is looking at buying U.K. startup for $1 billion')\n",
        "print(doc)\n",
        "\n",
        "for token in doc:\n",
        "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
        "          token.shape_, token.is_alpha, token.is_stop)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "teFo5lxGO181",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "4a95ca1a-3dc3-4ec3-8dc6-7b60315e26df"
      },
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(u'Apple is looking at buying U.K. startup for $1 billion')\n",
        "\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Apple 0 5 ORG\n",
            "U.K. 27 31 GPE\n",
            "$1 billion 44 54 MONEY\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oxN3NglbPndz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_lg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v2bIDn5KP1B5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b866f9b9-cdbd-4fbe-f7b8-8d77fb0dbe0a"
      },
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(u'I love coffee')\n",
        "print(doc.vocab.strings[u'coffee'])  # 3197928453018144401\n",
        "print(doc.vocab.strings[3197928453018144401])  # 'coffee'"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3197928453018144401\n",
            "coffee\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "b5zNURrtQV_U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(u'I love coffee')\n",
        "for word in doc:\n",
        "    lexeme = doc.vocab[word.text]\n",
        "    print(lexeme.text, lexeme.orth, lexeme.shape_, lexeme.prefix_, lexeme.suffix_,\n",
        "          lexeme.is_alpha, lexeme.is_digit, lexeme.is_title, lexeme.lang_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7Q6X1H1MU8Z1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Word Vectors and Similarity\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_md')\n",
        "doc = nlp(u\"Apple and banana are similar. Pasta and hippo aren't.\")\n",
        "\n",
        "apple = doc[0]\n",
        "banana = doc[2]\n",
        "pasta = doc[6]\n",
        "hippo = doc[8]\n",
        "\n",
        "print('apple <-> banana', apple.similarity(banana))\n",
        "print('pasta <-> hippo', pasta.similarity(hippo))\n",
        "print(apple.has_vector, banana.has_vector, pasta.has_vector, hippo.has_vector)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "72o4AyFNVROW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.matcher import Matcher\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "def set_sentiment(matcher, doc, i, matches):\n",
        "    doc.sentiment += 0.1\n",
        "\n",
        "pattern1 = [{'ORTH': 'Google'}, {'ORTH': 'I'}, {'ORTH': '/'}, {'ORTH': 'O'}]\n",
        "pattern2 = [[{'ORTH': emoji, 'OP': '+'}] for emoji in ['üòÄ', 'üòÇ', 'ü§£', 'üòç']]\n",
        "matcher.add('GoogleIO', None, pattern1) # match \"Google I/O\" or \"Google i/o\"\n",
        "matcher.add('HAPPY', set_sentiment, *pattern2) # match one or more happy emoji\n",
        "\n",
        "doc = nlp(u\"A text about Google I/O üòÄüòÄ\")\n",
        "matches = matcher(doc)\n",
        "\n",
        "for match_id, start, end in matches:\n",
        "   string_id = nlp.vocab.strings[match_id]\n",
        "   span = doc[start:end]\n",
        "   print(string_id, span.text)\n",
        "print('Sentiment', doc.sentiment)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G45UYBsToCFf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Uploading code file from local system\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "# choose the file on your computer to upload it then\n",
        "#import data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YKANyQZbogY4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Check if file upload is correct or not\n",
        "!cat testspcy.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t5Zo9yU8o3YY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Run the uploaded code file\n",
        "!python testspcy.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "isnoviQSqVoC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Uploading code file from local system\n",
        "from google.colab import files\n",
        "files.upload()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v9ujheE0qfXm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python testspcy2.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Le3wRKyBK_TD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Uploading code file from local system\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IzqN8TRGLJn3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cat testspcy2.py\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VIuMVOJULZzK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python testspcy2.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SaoptkGstXOZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#POS Tagging\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(u'Apple is looking at buying U.K. startup for $1 billion')\n",
        "\n",
        "for token in doc:\n",
        "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
        "          token.shape_, token.is_alpha, token.is_stop)\n",
        "\n",
        "# =============================================================================\n",
        "# Text: The original word text.\n",
        "# Lemma: The base form of the word.\n",
        "# POS: The simple part-of-speech tag.\n",
        "# Tag: The detailed part-of-speech tag.\n",
        "# Dep: Syntactic dependency, i.e. the relation between tokens.\n",
        "# Shape: The word shape ‚Äì capitalisation, punctuation, digits.\n",
        "# is alpha: Is the token an alpha character?\n",
        "# is stop: Is the token part of a stop list, i.e. the most common words of the language?\n",
        "# =============================================================================\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5CBfOP1itxAb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "h7FWUqXFtv0u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Noun Chunking\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(u\"Autonomous cars shift insurance liability toward manufacturers\")\n",
        "for chunk in doc.noun_chunks:\n",
        "    print(chunk.text, chunk.root.text, chunk.root.dep_,\n",
        "          chunk.root.head.text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4giF26udvSjq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Navigating the parse tree**"
      ]
    },
    {
      "metadata": {
        "id": "4CBaFY3wvRXZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(u\"Autonomous cars shift insurance liability toward manufacturers\")\n",
        "for token in doc:\n",
        "    print(token.text, token.dep_, token.head.text, token.head.pos_,\n",
        "          [child for child in token.children])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UPWooNmxv_cv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.symbols import nsubj, VERB\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(u\"Autonomous cars shift insurance liability toward manufacturers\")\n",
        "\n",
        "# Finding a verb with a subject from below ‚Äî good\n",
        "verbs = set()\n",
        "for possible_subject in doc:\n",
        "    if possible_subject.dep == nsubj and possible_subject.head.pos == VERB:\n",
        "        verbs.add(possible_subject.head)\n",
        "print(verbs)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dAwMkjCvwOHl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Iterating around the local tree**"
      ]
    },
    {
      "metadata": {
        "id": "h202b8rYMdwD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2plKedthwTWE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(u\"bright red apples on the tree\")\n",
        "print([token.text for token in doc[2].lefts])  # ['bright', 'red']\n",
        "print([token.text for token in doc[2].rights])  # ['on']\n",
        "print(doc[2].n_lefts)  # 2\n",
        "print(doc[2].n_rights)  # 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i9aZuUIdwm7i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(u\"Credit and mortgage account holders must submit their requests\")\n",
        "\n",
        "root = [token for token in doc if token.head == token][0]\n",
        "subject = list(root.lefts)[0]\n",
        "for descendant in subject.subtree:\n",
        "    assert subject is descendant or subject.is_ancestor(descendant)\n",
        "    print(descendant.text, descendant.dep_, descendant.n_lefts,\n",
        "          descendant.n_rights,\n",
        "          [ancestor.text for ancestor in descendant.ancestors])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L_9THGw9xLUx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Visualizing\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(u\"Autonomous cars shift insurance liability toward manufacturers\")\n",
        "displacy.render(doc, style='dep', jupyter=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CKjeJvY1yIqw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Named Entity Recognition 101**"
      ]
    },
    {
      "metadata": {
        "id": "6zjg3xRNyIP7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(u'Apple is looking at buying U.K. startup for $1 billion')\n",
        "\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ady5ziC3yedc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Accessing entity annotations**"
      ]
    },
    {
      "metadata": {
        "id": "w8Yg_RKGyd7m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(u'San Francisco considers banning sidewalk delivery robots')\n",
        "\n",
        "# document level\n",
        "ents = [(e.text, e.start_char, e.end_char, e.label_) for e in doc.ents]\n",
        "print(ents)\n",
        "\n",
        "# token level\n",
        "ent_san = [doc[0].text, doc[0].ent_iob_, doc[0].ent_type_]\n",
        "ent_francisco = [doc[1].text, doc[1].ent_iob_, doc[1].ent_type_]\n",
        "print(ent_san)  # [u'San', u'B', u'GPE']\n",
        "print(ent_francisco)  # [u'Francisco', u'I', u'GPE']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YcBHdZgJzWRT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#!python -m spacy download custom_ner_model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0asGZq-10jrI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python -m spacy validate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gRJSh8TQzwH_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cd /usr/local/lib/python3.6/dist-packages/spacy\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cLHQaEPHz-EA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls -ltr /usr/local/lib/python3.6/dist-packages/spacy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2TOJGlkDsXWl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Uploading prhase matcher\n",
        "#Uploading code file from local system\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MsUjdCKutf7W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en')\n",
        "doc = nlp('Hello     World!')\n",
        "for token in doc:\n",
        "    print('\"' + token.text + '\"')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CydX91JJz4dC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en')\n",
        "doc = nlp('Hello     World!')\n",
        "for token in doc:\n",
        "    print('\"' + token.text + '\"', token.idx)\n",
        " \n",
        "# \"Hello\" 0\n",
        "# \"    \" 6\n",
        "# \"World\" 10\n",
        "# \"!\" 15"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kC15K-z00HRP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "doc = nlp(\"Next week I'll   be in Madrid.\")\n",
        "for token in doc:\n",
        "    print(\"{0}\\t{1}\\t{2}\\t{3}\\t{4}\\t{5}\\t{6}\\t{7}\".format(\n",
        "        token.text,\n",
        "        token.idx,\n",
        "        token.lemma_,\n",
        "        token.is_punct,\n",
        "        token.is_space,\n",
        "        token.shape_,\n",
        "        token.pos_,\n",
        "        token.tag_\n",
        "    ))\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pO6fVMGM0SkY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Sentence detection**"
      ]
    },
    {
      "metadata": {
        "id": "mvPB-m3s0VAG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "doc = nlp(\"These are apples. These are oranges.\")\n",
        " \n",
        "for sent in doc.sents:\n",
        "    print(sent)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9uqVtZb20cw7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Part Of Speech Tagging**"
      ]
    },
    {
      "metadata": {
        "id": "GesZgP920cLb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "doc = nlp(\"Next week I'll be in Madrid.\")\n",
        "print([(token.text, token.tag_) for token in doc])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D8yme6g50o8R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Named Entity Recognition\n",
        "bold text"
      ]
    },
    {
      "metadata": {
        "id": "hv7qHABY0lPp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "doc = nlp(\"Next week I'll be in Madrid.\")\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_KPXQMDE06Vb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Entity Types**"
      ]
    },
    {
      "metadata": {
        "id": "nlWj47qI057v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "doc = nlp(\"I just bought 2 shares at 9 a.m. because the stock went up 30% in just 2 days according to the WSJ\")\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3iDqlDnf1Iau",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**displayCY**"
      ]
    },
    {
      "metadata": {
        "id": "xd7Sl18P1RXt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from spacy import displacy\n",
        " \n",
        "doc = nlp('I just bought 2 shares at 9 a.m. because the stock went up 30% in just 2 days according to the WSJ')\n",
        "displacy.render(doc, style='ent', jupyter=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LxnsLusB1aNF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Chunking**"
      ]
    },
    {
      "metadata": {
        "id": "J91pp69X1cfc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "doc = nlp(\"Wall Street Journal just published an interesting piece on crypto currencies\")\n",
        "for chunk in doc.noun_chunks:\n",
        "    print(chunk.text, chunk.label_, chunk.root.text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SPbYEDQQ1p8T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Dependency Parsing**"
      ]
    },
    {
      "metadata": {
        "id": "Nx9RBkDI1yS5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "doc = nlp('Wall Street Journal just published an interesting piece on crypto currencies')\n",
        " \n",
        "for token in doc:\n",
        "    print(\"{0}/{1} <--{2}-- {3}/{4}\".format(\n",
        "        token.text, token.tag_, token.dep_, token.head.text, token.head.tag_))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q5_9crNF1xyt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "St2LWMMb2ver",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**If this doesn‚Äôt help visualizing the dependency tree, displaCy comes in handy:**"
      ]
    },
    {
      "metadata": {
        "id": "zW6PxqGi2yf8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from spacy import displacy\n",
        " \n",
        "doc = nlp('Wall Street Journal just published an interesting piece on crypto currencies')\n",
        "displacy.render(doc, style='dep', jupyter=True, options={'distance': 90})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4DbXnGEj5W06",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Word Vectors**"
      ]
    },
    {
      "metadata": {
        "id": "H8NuuXAv53D6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_lg\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JtD60Hxt6c1Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_lg')\n",
        "print(nlp.vocab['banana'].vector)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B75XCbdi6qVQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from scipy import spatial\n",
        " \n",
        "cosine_similarity = lambda x, y: 1 - spatial.distance.cosine(x, y)\n",
        " \n",
        "man = nlp.vocab['man'].vector\n",
        "woman = nlp.vocab['woman'].vector\n",
        "queen = nlp.vocab['queen'].vector\n",
        "king = nlp.vocab['king'].vector\n",
        " \n",
        "# We now need to find the closest vector in the vocabulary to the result of \"man\" - \"woman\" + \"queen\"\n",
        "maybe_king = man - woman + queen\n",
        "computed_similarities = []\n",
        " \n",
        "for word in nlp.vocab:\n",
        "    # Ignore words without vectors\n",
        "    if not word.has_vector:\n",
        "        continue\n",
        " \n",
        "    similarity = cosine_similarity(maybe_king, word.vector)\n",
        "    computed_similarities.append((word, similarity))\n",
        " \n",
        "computed_similarities = sorted(computed_similarities, key=lambda item: -item[1])\n",
        "print([w[0].text for w in computed_similarities[:10]])\n",
        " \n",
        "# ['Queen', 'QUEEN', 'queen', 'King', 'KING', 'king', 'KIng', 'KINGS', 'kings', 'Kings']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3SmhXqaB7Tjm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Computing Similarity**"
      ]
    },
    {
      "metadata": {
        "id": "IgYLkuW87ZPa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "banana = nlp.vocab['banana']\n",
        "dog = nlp.vocab['dog']\n",
        "fruit = nlp.vocab['fruit']\n",
        "animal = nlp.vocab['animal']\n",
        " \n",
        "print(dog.similarity(animal), dog.similarity(fruit)) # 0.6618534 0.23552845\n",
        "print(banana.similarity(fruit), banana.similarity(animal)) # 0.67148364 0.2427285"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-5TzfMWI7kiP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "target = nlp(\"Cats are beautiful animals.\")\n",
        " \n",
        "doc1 = nlp(\"Dogs are awesome.\")\n",
        "doc2 = nlp(\"Some gorgeous creatures are felines.\")\n",
        "doc3 = nlp(\"Dolphins are swimming mammals.\")\n",
        " \n",
        "print(target.similarity(doc1))  # 0.8901765218466683\n",
        "print(target.similarity(doc2))  # 0.9115828449161616\n",
        "print(target.similarity(doc3))  # 0.7822956752876101"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wIXwFYmh72Ag",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Creating Document level Extension**"
      ]
    },
    {
      "metadata": {
        "id": "AGcaJR4z8InW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -U nltk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t5wwmpR18NlP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('vader_lexicon')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pfZlg6Fj71rs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.tokens import Doc\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        " \n",
        "sentiment_analyzer = SentimentIntensityAnalyzer()\n",
        "def polarity_scores(doc):\n",
        "    return sentiment_analyzer.polarity_scores(doc.text)\n",
        " \n",
        "Doc.set_extension('polarity_scores', getter=polarity_scores)\n",
        " \n",
        "nlp = spacy.load('en')\n",
        "doc = nlp(\"Really Whaaat event apple nice! it!\")\n",
        "print(doc._.polarity_scores)\n",
        "# {'neg': 0.0, 'neu': 0.596, 'pos': 0.404, 'compound': 0.5242}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dc9RNWZz9zq0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VAaNsGnF9sZ1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet as wn\n",
        "from spacy.tokens import Token\n",
        " \n",
        " \n",
        "def penn_to_wn(tag):\n",
        "    if tag.startswith('N'):\n",
        "        return 'n'\n",
        " \n",
        "    if tag.startswith('V'):\n",
        "        return 'v'\n",
        " \n",
        "    if tag.startswith('J'):\n",
        "        return 'a'\n",
        " \n",
        "    if tag.startswith('R'):\n",
        "        return 'r'\n",
        " \n",
        "    return None\n",
        " \n",
        " \n",
        "class WordnetPipeline(object):\n",
        "    def __init__(self, nlp):\n",
        "        Token.set_extension('synset',force=True)\n",
        " \n",
        "    def __call__(self, doc):\n",
        "        for token in doc:\n",
        "            wn_tag = penn_to_wn(token.tag_)\n",
        "            if wn_tag is None:\n",
        "                continue\n",
        " \n",
        "            ss = wn.synsets(token.text, wn_tag)[0]\n",
        "            token._.set('synset', ss)\n",
        " \n",
        "        return doc\n",
        " \n",
        " \n",
        "nlp = spacy.load('en')\n",
        "wn_pipeline = WordnetPipeline(nlp)\n",
        "nlp.add_pipe(wn_pipeline, name='wn_synsets')\n",
        "doc = nlp(\"Paris is the awesome capital of France.\")\n",
        " \n",
        "for token in doc:\n",
        "    print(token.text, \"-\", token._.synset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GQTQm62ibcVt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Uploading prhase matcher\n",
        "#Uploading code file from local system\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tod7u-Yzdc67",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mv 'test (1).text' test.text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vN_zNThNbNsA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Reading from a file\n",
        "text = open('test.text', 'r').read() # open a document\n",
        "doc = nlp(text) # process it\n",
        "print(doc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DekAGboIcw1k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!rm testspcy.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_tts2uu1eOyd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python testspcy.py"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}